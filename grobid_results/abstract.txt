Health workers (HW) are on the frontline fighting against the COVID-19 pandemic, they are exposed to multiple occupational hazards. This article analyzed the comprehensive measures of protecting HWs during the COVID-19 response in China. Occupational health protection of HWs was one of the key strategies of the public health measures adopted against the COVID-19 outbreak from the earliest stage in China. This prioritization of HWs health protection was based on the technical and policy guidance of WHO and International Labor Organization as well as the experiences from previous outbreaks in China. The comprehensive measures in China can be summarized as '6P-approach': public health emergency response, prompt learning from lessons, proactive measures of occupational health, precaution strategies against occupational hazards, personal protective equipment and medical devices supply, and professional networking. Through this 6P-approach, China was able to minimize the incidence of COVID-19 infection among HWs, while successfully containing the outbreak during the first quarter of 2020. Although the COVID-19 vaccines have been rolled out, however, the COVID-19 pandemic is still under rapidly evolving situation. Experiences from China may provide other countries with an example of prioritizing and incorporating occupational health protection of HWs in their public health measures responding to the COVID-19 pandemic.
The purpose of this study is to discuss about utilization of Whatsapp application as discussion media in Blended Learning. This study sought to integrate learning technologies to improve the quality of student's learning. Messenger application is used to communicate synchronously, so it can be positioned as a discussion media. This study focused on Whatsapp capabilities can be utilized to conduct in online learning of Blended Learning. This study aims to describe the discussion media. This study through the literature review that outlines the stages of Blended Learning using Whatsapp application as a discussion media by using qualitative methods. The result shows that Whatsapp application utilization as a discussion media in Blended Learning sessions initiated by offline using conventional methods, so the online session focused on the discussion as indicated by dialogue and interaction among participants.
Background: People with mental disorders in low-income countries are at risk of being left behind during efforts to expand universal health coverage.
Aims:To propose context-relevant strategies for moving towards universal health coverage for people with mental disorders in Ethiopia.
Methods:We conducted a situational analysis to inform a SWOT analysis of coverage of mental health services and financial risk protection, health system characteristics and the macroeconomic and fiscal environment. In-depth interviews were conducted with five national experts on health financing and equity and analysed using a thematic approach. Findings from the situation analysis and qualitative study were used to develop recommended strategies for adequate, fair and sustainable financing of mental health care in Ethiopia.
Results:Opportunities for improved financing of mental health care identified from the situation analysis included: a significant mental health burden with evidence from strong local epidemiological data; political commitment to address that burden; a health system with mechanisms for integrating mental health into primary care; and a favourable macro-fiscal environment for investment in human capabilities. Balanced against this were constraints of low current general government health expenditure, low numbers of mental health specialists, weak capacity to plan and implement mental health programmes and low population demand for mental health care. All key informants referred to the under-investment in mental health care in Ethiopia. Respondents emphasised opportunities afforded by positive rates of economic growth in the country and the expansion of community-based health insurance, as well as the need to ensure full implementation of existing task-sharing programmes for mental health care, integrate mental health into other priority programmes and strengthen advocacy to ensure mental health is given due attention.
Conclusion:Expansion of public health insurance, leveraging resources from high-priority SDG-related programmes and implementing existing plans to support task-shared mental health care are key steps towards universal health coverage for mental disorders in Ethiopia. However, external donors also need to deliver on commitments to include mental health within development funding. Future researchers and planners can apply this approach to other countries of sub-Saharan Africa and identify common strategies for sustainable and equitable financing of mental health care.
Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.
This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.
The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result-for example, how sensitive a prediction of zebra is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.
In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of interpretability and explanation. The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.
depth, the performance of the input feature extraction is also enhanced. For example, AlexNet -one of the most representative CNNs, has 650K neurons and 60M related parameters [36]. Also, sophisticated algorithms are proposed to support the training and testing of such a complex network, and the backpropagation method is widely applied to train the CNN parameters through multiple layers [39,44]. Furthermore, to fine-tune the network to specific functions, large pools of labeled data are required for iteratively training the massive neurons and connection weights. So far, many high-performance CNN designs have been proposed, such as AlexNet [36], VGG [63], GoogleNet [66], ResNet [27], etc. Some of the designs can even achieve beyond human-level accuracy on object recognition tasks [61].Although the CNNs can achieve competitive classification accuracy, the CNNs still suffer from high computational cost, slow training speed, and security vulnerability [67,38,4]. One major reason causing these shortcomings is the lack of network interpretability, especially the limited understanding of the internal features learned by each convolutional layer: Mathematically, the convolutional layer neurons (namely the convolution filters) convolve with the input image or the outputs of the previous layer, the results are considered as learned features and recorded in the feature maps. With deeper layers, the neurons are expected to extract higher level features, and eventually converge to the final classification. However, as the CNN training is considered as a black-box process and the neurons are designed in the format of simple matrices, the formation of those neuron values are unpredictable and the neuron meanings are impossible to directly explained. Hence, the poor network interpretability significantly hinders the robustness evaluation of each network layer, the further optimization on the network structure, as well as the network adaptability and transferability to different applications [53,60].A qualitative way to improve the network interpretability is the network visualization, which translates the internal features into visually perceptible image patterns. This visualization process is referred from the human visual cortex system analysis: In a human brain, the human visual cortex is embedded in multiple vision neuron areas [57]. In each vision neuron area, numerous neurons selectively respond to different features, such as colors, edges, and shapes [35,54]. To explore the relationship between the neurons and features, researchers usually find the preferred stimulus to identify individual kind of the response and illustrate the response to certain visual patterns. The CNN visualization also follows such an analytical approach to realize the CNN interpretability.Up to now, many effective network visualization works have been proposed in the literature, and several representative methods are widely adopted: 1) Erhan et al. proposed the Activation Maximization to interpret traditional shallow networks [14,28,68]. Later, this method was further improved by Simonyan et al., which synthesized an input image pattern with the maximum activation of a single CNN neuron for visualization [62]. This fundamental method was also extended by many other works with different regularizers for interpretability improvement of the synthesized image patterns [71,52,50]. 2) Besides the visualization of a single neuron, Mahendran et al. revealed the CNN internal features in the layer level [46,47]. The Network Inversion was proposed to reconstruct an input image based on multiple neurons' activation to illustrate a comprehensive feature map learned by each single CNN layer. 3) Rather than reconstructing an input image for feature visualization, Zeiler et al. proposed the Deconvolutional Neural Network based Visualization (DeconvNet) [72], which utilized the DeconvNet framework to
There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.
A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) [ ] or Feature Visualization via Optimization. In this chapter, we ( ) review existing AM techniques in the literature; ( ) discuss a probabilistic interpretation for AM; and ( ) review the applications of AM in debugging and explaining networks.
Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.
Disobeying the classical wisdom of statistical learning theory, modern deep neural networks generalize well even though they typically contain millions of parameters. Recently, it has been shown that the trajectories of iterative optimization algorithms can possess fractal structures, and their generalization error can be formally linked to the complexity of such fractals. This complexity is measured by the fractal's intrinsic dimension, a quantity usually much smaller than the number of parameters in the network. Even though this perspective provides an explanation for why overparametrized networks would not overfit, computing the intrinsic dimension (e.g., for monitoring generalization during training) is a notoriously difficult task, where existing methods typically fail even in moderate ambient dimensions. In this study, we consider this problem from the lens of topological data analysis (TDA) and develop a generic computational tool that is built on rigorous mathematical foundations. By making a novel connection between learning theory and TDA, we first illustrate that the generalization error can be equivalently bounded in terms of a notion called the 'persistent homology dimension' (PHD), where, compared with prior work, our approach does not require any additional geometrical or statistical assumptions on the training dynamics. Then, by utilizing recently established theoretical results and TDA tools, we develop an efficient algorithm to estimate PHD in the scale of modern deep neural networks and further provide visualization tools to help understand generalization in deep learning. Our experiments show that the proposed approach can efficiently compute a network's intrinsic dimension in a variety of settings, which is predictive of the generalization error.
Figure 1. Generating high-fidelity 512 2 images in a single step. All samples are generated with a single U-Net evaluation trained with adversarial diffusion distillation (ADD).
Deep learning (DL) has been successfully applied to different fields for a range of tasks. In medicine, DL methods have been also used to improve the efficiency of disease diagnosis. In this review, we first summarize the history of the development of artificial intelligence models, demonstrate the features of the subtypes of machine learning and different DL networks, and then explore their application in the different fields of precision medicine, such as cardiology, gastroenterology, ophthalmology, dermatology, and oncology. By digging more information and extracting multilevel features from medical data, we found that DL helps doctors assess diseases automatically and monitor patients' physical health. In gliomas, research regarding application prospect of DL was mainly shown through magnetic resonance imaging and then by pathological slides. However, multi-omics data, such as whole exome sequence, RNA sequence, proteomics, and epigenomics, have not been covered thus far. In general, the quality and quantity of DL datasets still need further improvements, and more fruitful multi-omics characteristics will bring more comprehensive and accurate diagnosis in precision medicine and glioma.
State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.
Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one-and few-step sampling, achieving the new state-ofthe-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64 ˆ64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64 ˆ64 and LSUN 256 ˆ256.
Public health measures necessary to counteract the coronavirus disease 2019 (COVID-19) pandemic have resulted in dramatic changes in the physical and social environments within which children grow and develop. As our understanding of the pathways for viral exposure and associated health outcomes in children evolves, it is critical to consider how changes in the social, cultural, economic, and physical environments resulting from the pandemic could affect the development of children. This review article considers the environments and settings that create the backdrop for children's health in the United States during the COVID-19 pandemic, including current threats to child development that stem from: A) change in exposures to environmental contaminants such as heavy metals, pesticides, disinfectants, air pollution and the built environment; B) changes in food environments resulting from adverse economic repercussion of the pandemic and limited reach of existing safety nets; C) limited access to children's educational and developmental resources; D) changes in the social environments at the individual and household levels, and their interplay with family stressors and mental health; E) social injustice and racism. The environmental changes due to COVID-19 are overlaid onto existing environmental and social disparities. This results in disproportionate effects among children in low-income settings and among populations experiencing the effects of structural racism. This article draws attention to many environments that should be considered in current and future policy responses to protect children's health amid pandemics.
Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.
We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have nearrandom accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.
Latent Consistency Models (LCMs) (Luo et al., 2023) have achieved impressive performance in accelerating text-to-image generative tasks, producing highquality images with minimal inference steps. LCMs are distilled from pretrained latent diffusion models (LDMs), requiring only ∼32 A100 GPU training hours. This report further extends LCMs' potential in two aspects: First, by applying LoRA distillation to Stable-Diffusion models including SD-V1.5 (Rombach et al., 2022), SSD-1B (Segmind., 2023), and SDXL (Podell et al., 2023), we have expanded LCM's scope to larger models with significantly less memory consumption, achieving superior image generation quality. Second, we identify the LoRA parameters obtained through LCM distillation as a universal Stable-Diffusion acceleration module, named LCM-LoRA. LCM-LoRA can be directly plugged into various Stable-Diffusion fine-tuned models or LoRAs without training, thus representing a universally applicable accelerator for diverse image generation tasks. Compared with previous numerical PF-ODE solvers such as DDIM (Song et al., 2020), DPM-Solver (Lu et al., 2022a;b), LCM-LoRA can be viewed as a plug-in neural PF-ODE solver that possesses strong generalization abilities. Project page: https://github.com/luosiallen/ latent-consistency-model.
In this study, we estimated the risk of acute coronary syndrome and stroke associated with several emerging cardiovascular risk factors. This was a case-control study, where an age -and sex-matched acute coronary syndrome group and stroke group were compared with controls. Demographic and clinical data were collected through patient interviews, and blood samples were taken for analysis. In the bivariate analysis, all cardiovascular risk factors analyzed showed as predictors of acute coronary syndrome and stroke, except total cholesterol and smoking. In the multivariate logistic regression model for acute coronary syndrome, hypertension and body mass index, N-terminal section brain natriuretic peptide and pregnancy-associated plasma protein-A were independent predictors. For stroke, the predictors were hypertension, diabetes mellitus, body mass index, and N-terminal section brain natriuretic peptide. Controlling for age, sex, and classical cardiovascular risk factors, N-terminal section brain natriuretic peptide and pregnancy-associated plasma protein-A were independent emerging cardiovascular risk factors for acute coronary syndrome, but pregnancy-associated plasma protein-A was not for stroke. High levels of cardiovascular risk factors in individuals with no episodes of cardiovascular disease requires the implementation of prevention programs, given that at least half of them are modifiable.
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.

Introduction:Many hours of a student's life are spent in a lecture hall, "the supposed place of education, growth, and understanding." Likewise, countless hours are spent by faculty and staff in preparation for these lectures. The objective of the study was to discover the perceptions of medical students on current trends in lecture delivery. The study also arises from the fact that there is a dearth of data in Mauritius.
Methods:A cross-sectional observational analytic pilot study was conducted via the method of a semi-structured closed-ended questionnaire. A 5-point Likert scale (strongly disagree, disagree, neither agree nor disagree, agree, strongly agree) was used to record the responses. Two male and two female students were chosen via a convenience sampling technique from all the semesters viz. Semester one to semester ten.
Results:Lectures reduced anxiety with regards to study material was "strongly agreed" by 46.4% Indians, whereas 62.5% of South Africans and 50% Mauritians opted the "agree" option. As far as faculty as role models were concerned, 15 (37.5%) of students [males 7 (35%); females 8 (40%)] strongly agreed with the option. Preference of structural approach and the correlation with gender was found to be statistically significant P <0.05.
Conclusion:The data both suggest and prove that students are ultimately still in favor of lectures; however, duration, content, and lecture formats were critically appraised. Although the study is a pilot study, which was conducted on a small sample size, the findings can be utilized as baseline data for further investigations.
We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model finetuned to follow instructions, Mixtral 8x7B -Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B -chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. * Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.† Work performed while at Google Brain.‡ Work performed while at Google Research.
Two newly proposed Directives impact liability for artificial intelligence in the EU: a Product Liability Directive (PLD) and an AI Liability Directive (AILD). While these proposed Directives provide some uniform liability rules for AI-caused harm, they fail to fully accomplish the EU's goal of providing clarity and uniformity for liability for injuries caused by AI-driven goods and services. Instead, the Directives leave potential liability gaps for injuries caused by some black-box medical AI systems, which use opaque and complex reasoning to provide medical decisions and/or recommendations. Patients may not be able to successfully sue manufacturers or healthcare providers for some injuries caused by these black-box medical AI systems under either EU Member States' strict or fault-based liability laws. Since the proposed Directives fail to address these potential liability gaps, manufacturers and healthcare providers may have difficulty predicting liability risks associated with creating and/or using some potentially beneficial black-box medical AI systems.
The COVID-19 pandemic caused by the novel coronavirus SARS-CoV-2 has led to over 910,000 deaths worldwide and unprecedented decimation of the global economy. Despite its tremendous impact, the origin of SARS-CoV-2 has remained mysterious and controversial. The natural origin theory, although widely accepted, lacks substantial support. The alternative theory that the virus may have come from a research laboratory is, however, strictly censored on peer-reviewed scientific journals. Nonetheless, SARS-CoV-2 shows biological characteristics that are inconsistent with a naturally occurring, zoonotic virus. In this report, we describe the genomic, structural, medical, and literature evidence, which, when considered together, strongly contradicts the natural origin theory. The evidence shows that SARS-CoV-2 should be a laboratory product created by using bat coronaviruses ZC45 and/or ZXC21 as a template and/or backbone. Building upon the evidence, we further postulate a synthetic route for SARS-CoV-2, demonstrating that the laboratory-creation of this coronavirus is convenient and can be accomplished in approximately six months. Our work emphasizes the need for an independent investigation into the relevant research laboratories. It also argues for a critical look into certain recently published data, which, albeit problematic, was used to support and claim a natural origin of SARS-CoV-2. From a public health perspective, these actions are necessary as knowledge of the origin of SARS-CoV-2 and of how the virus entered the human population are of pivotal importance in the fundamental control of the COVID-19 pandemic as well as in preventing similar, future pandemics.
