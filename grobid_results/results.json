[
  {
    "file": "1-s20-S2414644721000944-main.pdf",
    "title": "Global Health Journal",
    "abstract": "Health workers (HW) are on the frontline fighting against the COVID-19 pandemic, they are exposed to multiple occupational hazards. This article analyzed the comprehensive measures of protecting HWs during the COVID-19 response in China. Occupational health protection of HWs was one of the key strategies of the public health measures adopted against the COVID-19 outbreak from the earliest stage in China. This prioritization of HWs health protection was based on the technical and policy guidance of WHO and International Labor Organization as well as the experiences from previous outbreaks in China. The comprehensive measures in China can be summarized as '6P-approach': public health emergency response, prompt learning from lessons, proactive measures of occupational health, precaution strategies against occupational hazards, personal protective equipment and medical devices supply, and professional networking. Through this 6P-approach, China was able to minimize the incidence of COVID-19 infection among HWs, while successfully containing the outbreak during the first quarter of 2020. Although the COVID-19 vaccines have been rolled out, however, the COVID-19 pandemic is still under rapidly evolving situation. Experiences from China may provide other countries with an example of prioritizing and incorporating occupational health protection of HWs in their public health measures responding to the COVID-19 pandemic.",
    "acknowledgment": "The authors would like to thank Dr. Ivan D. Ivanov (Occupational and Workplace Health, WHO headquarter) for his professional comments to this article."
  },
  {
    "file": "10.pdf",
    "title": "Utilization of whatsapp application as discussion media in blended learning",
    "abstract": "The purpose of this study is to discuss about utilization of Whatsapp application as discussion media in Blended Learning. This study sought to integrate learning technologies to improve the quality of student's learning. Messenger application is used to communicate synchronously, so it can be positioned as a discussion media. This study focused on Whatsapp capabilities can be utilized to conduct in online learning of Blended Learning. This study aims to describe the discussion media. This study through the literature review that outlines the stages of Blended Learning using Whatsapp application as a discussion media by using qualitative methods. The result shows that Whatsapp application utilization as a discussion media in Blended Learning sessions initiated by offline using conventional methods, so the online session focused on the discussion as indicated by dialogue and interaction among participants.",
    "acknowledgment": "This research was supported by Lembaga Penelitian dan Pengabdian Masyarakat (LPPM) Kanjuruhan University. We thank our colleagues from Mathematics Education Department also Science and Technology Faculty of Kanjuruhan University who provided insight and expertise that greatly assisted the research, although they may not agree with all of the conclusions of this paper."
  },
  {
    "file": "13033_2019_Article_268.pdf",
    "title": "Moving towards universal health coverage for mental disorders in Ethiopia",
    "abstract": "Background: People with mental disorders in low-income countries are at risk of being left behind during efforts to expand universal health coverage.\nAims:To propose context-relevant strategies for moving towards universal health coverage for people with mental disorders in Ethiopia.\nMethods:We conducted a situational analysis to inform a SWOT analysis of coverage of mental health services and financial risk protection, health system characteristics and the macroeconomic and fiscal environment. In-depth interviews were conducted with five national experts on health financing and equity and analysed using a thematic approach. Findings from the situation analysis and qualitative study were used to develop recommended strategies for adequate, fair and sustainable financing of mental health care in Ethiopia.\nResults:Opportunities for improved financing of mental health care identified from the situation analysis included: a significant mental health burden with evidence from strong local epidemiological data; political commitment to address that burden; a health system with mechanisms for integrating mental health into primary care; and a favourable macro-fiscal environment for investment in human capabilities. Balanced against this were constraints of low current general government health expenditure, low numbers of mental health specialists, weak capacity to plan and implement mental health programmes and low population demand for mental health care. All key informants referred to the under-investment in mental health care in Ethiopia. Respondents emphasised opportunities afforded by positive rates of economic growth in the country and the expansion of community-based health insurance, as well as the need to ensure full implementation of existing task-sharing programmes for mental health care, integrate mental health into other priority programmes and strengthen advocacy to ensure mental health is given due attention.\nConclusion:Expansion of public health insurance, leveraging resources from high-priority SDG-related programmes and implementing existing plans to support task-shared mental health care are key steps towards universal health coverage for mental disorders in Ethiopia. However, external donors also need to deliver on commitments to include mental health within development funding. Future researchers and planners can apply this approach to other countries of sub-Saharan Africa and identify common strategies for sustainable and equitable financing of mental health care.",
    "acknowledgment": "The authors would like to acknowledge the contribution of Caroline Whidden to the first draft of the Ethiopia situational assessment."
  },
  {
    "file": "1506.02753.pdf",
    "title": "Inverting Visual Representations with Convolutional Networks",
    "abstract": "Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.",
    "acknowledgment": "We acknowledge funding by the ERC Starting Grant VideoLearn (279401). We are grateful to Aravindh Mahendran for sharing with us the reconstructions achieved with the method of Mahendran and Vedaldi [19]. We thank Jost Tobias Springenberg for comments."
  },
  {
    "file": "1706.07979.pdf",
    "title": "Methods for Interpreting and Understanding Deep Neural Networks",
    "abstract": "This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.",
    "acknowledgment": "We gratefully acknowledge discussions and comments on the manuscript by our colleagues Sebastian Lapuschkin, and Alexander Binder. This work was supported by the Brain Korea 21 Plus Program through the National Research Foundation of Korea; the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government [No. 2017-0-00451]; the Deutsche Forschungsgemeinschaft (DFG) [grant MU 987/17-1]; and the German Ministry for Education and Research as Berlin Big Data Center (BBDC) [01IS14013A]. This publication only reflects the authors views. Funding agencies are not liable for any use that may be made of the information contained herein."
  },
  {
    "file": "1711.11279.pdf",
    "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
    "abstract": "The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result-for example, how sensitive a prediction of zebra is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.",
    "acknowledgment": "We would like to thank Daniel Smilkov for helpful discussions. We thank Alexander Mordvintsev for providing tfzoo code. We also thank Ethan R Elenberg, David Alvarez Melis and an anonymous reviewer for helpful comments and discussions. We thank Alexander Mordvintsev, Chris Olah and Ludwig Schubert for generously allowing us to use their code for DeepDream. Thanks to Christopher for sharing early work on doing attribution to semantically meaningful channels. Work from Nicholas Carlini, on training linear classifiers for non-label concepts on logit-layer activations, was one of our motivations. Finally, we would like to thank Dr. Zahra Rastegar for evaluating diabetic retinopathy results, and provided relevant medical expertise."
  },
  {
    "file": "1802.01933.pdf",
    "title": "A Survey Of Methods For Explaining Black Box Models",
    "abstract": "In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of interpretability and explanation. The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.",
    "acknowledgment": "This work is partially supported by the European Community's H2020 Program under the funding scheme \"INFRAIA-1-2014-2015: Research Infrastructures\", grant agreement 654024, SoBigData, http://www.sobigdata.eu."
  },
  {
    "file": "1804.11191.pdf",
    "title": "Mathematical Foundations of Computing",
    "abstract": "",
    "acknowledgment": "Acknowledgments. This work was supported in part by NSF CNS-1717775."
  },
  {
    "file": "1806.00069.pdf",
    "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
    "abstract": "There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",
    "acknowledgment": "The work was partially funded by DARPA XAI program FA8750-18-C0004, the National Science Foundation under Grants No. 1524817, the MIT-IBM Watson AI Lab, and the Toyota Research Institute (TRI). The authors also wish to express their appreciation for Jonathan Frankle for sharing his insightful feedback on earlier versions of the manuscript."
  },
  {
    "file": "1904.08939.pdf",
    "title": "Understanding Neural Networks via Feature Visualization: A survey",
    "abstract": "A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) [ ] or Feature Visualization via Optimization. In this chapter, we ( ) review existing AM techniques in the literature; ( ) discuss a probabilistic interpretation for AM; and ( ) review the applications of AM in debugging and explaining networks.",
    "acknowledgment": "Acknowledgments Anh Nguyen is supported by Amazon Research Credits, Auburn University, and donations from Adobe Systems Inc and Nvidia."
  },
  {
    "file": "2006.00093.pdf",
    "title": "Explainable Artificial Intelligence: a Systematic Review",
    "abstract": "Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.",
    "acknowledgment": null
  },
  {
    "file": "2111.13171.pdf",
    "title": "Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks",
    "abstract": "Disobeying the classical wisdom of statistical learning theory, modern deep neural networks generalize well even though they typically contain millions of parameters. Recently, it has been shown that the trajectories of iterative optimization algorithms can possess fractal structures, and their generalization error can be formally linked to the complexity of such fractals. This complexity is measured by the fractal's intrinsic dimension, a quantity usually much smaller than the number of parameters in the network. Even though this perspective provides an explanation for why overparametrized networks would not overfit, computing the intrinsic dimension (e.g., for monitoring generalization during training) is a notoriously difficult task, where existing methods typically fail even in moderate ambient dimensions. In this study, we consider this problem from the lens of topological data analysis (TDA) and develop a generic computational tool that is built on rigorous mathematical foundations. By making a novel connection between learning theory and TDA, we first illustrate that the generalization error can be equivalently bounded in terms of a notion called the 'persistent homology dimension' (PHD), where, compared with prior work, our approach does not require any additional geometrical or statistical assumptions on the training dynamics. Then, by utilizing recently established theoretical results and TDA tools, we develop an efficient algorithm to estimate PHD in the scale of modern deep neural networks and further provide visualization tools to help understand generalization in deep learning. Our experiments show that the proposed approach can efficiently compute a network's intrinsic dimension in a variety of settings, which is predictive of the generalization error.",
    "acknowledgment": "Umut \u015eim\u015fekli's research is supported by the French government under management of Agence Nationale de la Recherche as part of the \"Investissements d'avenir\" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute)."
  },
  {
    "file": "2404.12810.pdf",
    "title": "Enhancing Counterfactual Explanation Search with Diffusion Distance and Directional Coherence",
    "abstract": "A pressing issue in the adoption of AI models is the increasing demand for more human-centric explanations of their predictions. To advance towards more human-centric explanations, understanding how humans produce and select explanations has been beneficial. In this work, inspired by insights of human cognition we propose and test the incorporation of two novel biases to enhance the search for effective counterfactual explanations. Central to our methodology is the application of diffusion distance, which emphasizes data connectivity and actionability in the search for feasible counterfactual explanations. In particular, diffusion distance effectively weights more those points that are more interconnected by numerous short-length paths. This approach brings closely connected points nearer to each other, identifying a feasible path between them. We also introduce a directional coherence term that allows the expression of a preference for the alignment between the joint and marginal directional changes in feature space to reach a counterfactual. This term enables the generation of counterfactual explanations that align with a set of marginal predictions based on expectations of how the outcome of the model varies by changing one feature at a time. We evaluate our method, named Coherent Directional Counterfactual Explainer (CoDiCE), and the impact of the two novel biases against existing methods such as DiCE, FACE, Prototypes, and Growing Spheres. Through a series of ablation experiments on both synthetic and real datasets with continuous and mixed-type features, we demonstrate the effectiveness of our method.",
    "acknowledgment": "Acknowledgments. This research was supported by the Estonian Research Council Grants PRG1604, the European Union's Horizon 2020 Research and Innovation Programme under Grant Agreement No. 952060 (Trust AI), by the Estonian Centre of Excellence in Artificial Intelligence (EXAI), by the Estonian Ministry of Education and Research."
  },
  {
    "file": "2404.15029.pdf",
    "title": "Explainable LightGBM Approach for Predicting Myocardial Infarction Mortality",
    "abstract": "Myocardial Infarction is a main cause of mortality globally, and accurate risk prediction is crucial for improving patient outcomes. Machine Learning techniques have shown promise in identifying high-risk patients and predicting outcomes. However, patient data often contain vast amounts of information and missing values, posing challenges for feature selection and imputation methods. In this article, we investigate the impact of the data preprocessing task and compare three ensembles boosted tree methods to predict the risk of mortality in patients with myocardial infarction. Further, we use the Tree Shapley Additive Explanations method to identify relationships among all the features for the performed predictions, leveraging the entirety of the available data in the analysis. Notably, our approach achieved a superior performance when compared to other existing machine learning approaches, with an F1-score of 91,2% and an accuracy of 91,8% for LightGBM without data preprocessing.",
    "acknowledgment": "VI. ACKNOWLEDGMENT This work was partially financed by the Coordenac \u00b8\u00e3o de Aperfeic \u00b8oamento de Pessoal de N\u00edvel Superior -Brasil (CAPES), and by Fundac \u00b8\u00e3o de Amparo \u00e0 Pesquisa do Estado de S\u00e3o Paulo (FAPESP) # 2023/06737-7 and INCT (CAPES #88887.136349/2017-00, CNPQ #465755/2014-3)."
  },
  {
    "file": "Adversarial-Diffusion-Distillation.pdf",
    "title": "Adversarial Diffusion Distillation",
    "abstract": "Figure 1. Generating high-fidelity 512 2 images in a single step. All samples are generated with a single U-Net evaluation trained with adversarial diffusion distillation (ADD).",
    "acknowledgment": "We would like to thank Jonas M\u00fcller for feedback on the draft, the proof, and typesetting; Patrick Esser for feedback on the proof and building an early model demo; Frederic Boesel for generating data and helpful discussions; Minguk Kang and Taesung Park for providing GigaGAN samples; Richard Vencu, Harry Saini, and Sami Kama for maintaining the compute infrastructure; Yara Wald for creative sampling support; and Vanessa Sauer for her general support."
  },
  {
    "file": "BioengineeringTranslaMed-2023-Liu.pdf",
    "title": "Deep learning in precision medicine and focus on glioma",
    "abstract": "Deep learning (DL) has been successfully applied to different fields for a range of tasks. In medicine, DL methods have been also used to improve the efficiency of disease diagnosis. In this review, we first summarize the history of the development of artificial intelligence models, demonstrate the features of the subtypes of machine learning and different DL networks, and then explore their application in the different fields of precision medicine, such as cardiology, gastroenterology, ophthalmology, dermatology, and oncology. By digging more information and extracting multilevel features from medical data, we found that DL helps doctors assess diseases automatically and monitor patients' physical health. In gliomas, research regarding application prospect of DL was mainly shown through magnetic resonance imaging and then by pathological slides. However, multi-omics data, such as whole exome sequence, RNA sequence, proteomics, and epigenomics, have not been covered thus far. In general, the quality and quantity of DL datasets still need further improvements, and more fruitful multi-omics characteristics will bring more comprehensive and accurate diagnosis in precision medicine and glioma.",
    "acknowledgment": "Center of Central South University."
  },
  {
    "file": "CLIP-Connecting-text-and-images.pdf",
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
    "acknowledgment": "We'd like to thank the millions of people involved in creating the data CLIP is trained on. We'd also like to thank Susan Zhang for her work on image conditional language models while at OpenAI, Ishaan Gulrajani for catching an error in the pseudocode, and Irene Solaiman, Miles Brundage, and Gillian Hadfield for their thoughtful feedback on the broader impacts section of the paper. We are also grateful to the Acceleration and Supercomputing teams at OpenAI for their critical work on software and hardware infrastructure this project used. Finally, we'd also like to thank the developers of the many software packages used throughout this project including, but not limited, to Numpy (Harris et al., 2020), SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-Flow (Abadi et al., 2016), PyTorch (Paszke et al., 2019), pandas (pandas development team, 2020), and scikit-learn (Pedregosa et al., 2011)."
  },
  {
    "file": "Consistency-Models.pdf",
    "title": "Consistency Models",
    "abstract": "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one-and few-step sampling, achieving the new state-ofthe-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64 \u02c664 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64 \u02c664 and LSUN 256 \u02c6256.",
    "acknowledgment": "We thank Alex Nichol for reviewing the manuscript and providing valuable feedback, Chenlin Meng for providing stroke inputs needed in our stroke-guided image generation experiments, and the OpenAI Algorithms team."
  },
  {
    "file": "covid19.pdf",
    "title": "COVID-19 and children's health in the United States: Consideration of physical and social environments during the pandemic",
    "abstract": "Public health measures necessary to counteract the coronavirus disease 2019 (COVID-19) pandemic have resulted in dramatic changes in the physical and social environments within which children grow and develop. As our understanding of the pathways for viral exposure and associated health outcomes in children evolves, it is critical to consider how changes in the social, cultural, economic, and physical environments resulting from the pandemic could affect the development of children. This review article considers the environments and settings that create the backdrop for children's health in the United States during the COVID-19 pandemic, including current threats to child development that stem from: A) change in exposures to environmental contaminants such as heavy metals, pesticides, disinfectants, air pollution and the built environment; B) changes in food environments resulting from adverse economic repercussion of the pandemic and limited reach of existing safety nets; C) limited access to children's educational and developmental resources; D) changes in the social environments at the individual and household levels, and their interplay with family stressors and mental health; E) social injustice and racism. The environmental changes due to COVID-19 are overlaid onto existing environmental and social disparities. This results in disproportionate effects among children in low-income settings and among populations experiencing the effects of structural racism. This article draws attention to many environments that should be considered in current and future policy responses to protect children's health amid pandemics.",
    "acknowledgment": "The authors would like to thank Jermesha Smith and Meghan Ryan, LCPC, for helpful discussions about working with families during the pandemic, and Eduardo Gonzalez for assisting with the referencing of this manuscript."
  },
  {
    "file": "DALL\u00b7E-Creating-images-from-text.pdf",
    "title": "Zero-Shot Text-to-Image Generation",
    "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
    "acknowledgment": "We would like to thank Matthew Knight for reviewing the code release for this work, and Rewon Child, John Schulman, Heewoo Jun, and Prafulla Dhariwal for helpful early feedback on the paper. We would also like to thank Jong Wook Kim for writing the PyTorch package for the contrastive model described in Radford et al. (2019) that we used to rerank the samples from our model."
  },
  {
    "file": "GEMINI-Measuring-Massive-Multitask-Language-Understanding.pdf",
    "title": "MEASURING MASSIVE MULTITASK LANGUAGE UNDERSTANDING",
    "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have nearrandom accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
    "acknowledgment": "We would like to thank the following for their helpful comments: Oyvind Tafjord, Jan Leike, David Krueger, Alex Tamkin, Girish Sastry, and Henry Zhu. DH is supported by the NSF GRFP Fellowship and an Open Philanthropy Project Fellowship. This research was also supported by the NSF Frontier Award 1804794."
  },
  {
    "file": "LCM-LoRA.pdf",
    "title": "LCM-LORA: A UNIVERSAL STABLE-DIFFUSION ACCELERATION MODULE",
    "abstract": "Latent Consistency Models (LCMs) (Luo et al., 2023) have achieved impressive performance in accelerating text-to-image generative tasks, producing highquality images with minimal inference steps. LCMs are distilled from pretrained latent diffusion models (LDMs), requiring only \u223c32 A100 GPU training hours. This report further extends LCMs' potential in two aspects: First, by applying LoRA distillation to Stable-Diffusion models including SD-V1.5 (Rombach et al., 2022), SSD-1B (Segmind., 2023), and SDXL (Podell et al., 2023), we have expanded LCM's scope to larger models with significantly less memory consumption, achieving superior image generation quality. Second, we identify the LoRA parameters obtained through LCM distillation as a universal Stable-Diffusion acceleration module, named LCM-LoRA. LCM-LoRA can be directly plugged into various Stable-Diffusion fine-tuned models or LoRAs without training, thus representing a universally applicable accelerator for diverse image generation tasks. Compared with previous numerical PF-ODE solvers such as DDIM (Song et al., 2020), DPM-Solver (Lu et al., 2022a;b), LCM-LoRA can be viewed as a plug-in neural PF-ODE solver that possesses strong generalization abilities. Project page: https://github.com/luosiallen/ latent-consistency-model.",
    "acknowledgment": null
  },
  {
    "file": "Linares_et_al-2016-Nursing_&_Health_Sciences.pdf",
    "title": "Association of cardiovascular emerging risk factors with acute coronary syndrome and stroke: A case-control study",
    "abstract": "In this study, we estimated the risk of acute coronary syndrome and stroke associated with several emerging cardiovascular risk factors. This was a case-control study, where an age -and sex-matched acute coronary syndrome group and stroke group were compared with controls. Demographic and clinical data were collected through patient interviews, and blood samples were taken for analysis. In the bivariate analysis, all cardiovascular risk factors analyzed showed as predictors of acute coronary syndrome and stroke, except total cholesterol and smoking. In the multivariate logistic regression model for acute coronary syndrome, hypertension and body mass index, N-terminal section brain natriuretic peptide and pregnancy-associated plasma protein-A were independent predictors. For stroke, the predictors were hypertension, diabetes mellitus, body mass index, and N-terminal section brain natriuretic peptide. Controlling for age, sex, and classical cardiovascular risk factors, N-terminal section brain natriuretic peptide and pregnancy-associated plasma protein-A were independent emerging cardiovascular risk factors for acute coronary syndrome, but pregnancy-associated plasma protein-A was not for stroke. High levels of cardiovascular risk factors in individuals with no episodes of cardiovascular disease requires the implementation of prevention programs, given that at least half of them are modifiable.",
    "acknowledgment": "This research was conducted with funding from the Health Agency of Health South of Grenada, Spain, and was partially funded by the MTM2013-47929-P project from \"Secretariat of State for Research, Development and Innovation, Ministry of Economy and Competitiveness\", Spain."
  },
  {
    "file": "Llama2-Open-Foundation-and-Fine-Tuned-Chat-Models.pdf",
    "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
    "acknowledgment": null
  },
  {
    "file": "mastodon_mammon_02.pdf",
    "title": "MASTODON OVER MAMMON Towards publicly owned scholarly knowledge",
    "abstract": "Twitter is in turmoil and the scholarly community on the platform is once again starting to migrate. As with the early internet, scholarly organizations are at the forefront of developing and implementing a decentralized alternative to Twitter, Mastodon. Both historically and conceptually, this is not a new situation for the scholarly community. Historically, scholars were forced to leave social media platform FriendFeed after it was bought by Facebook in 2006. Conceptually, the problems associated with public scholarly discourse subjected to the whims of corporate owners are not unlike those of scholarly journals owned by monopolistic corporations: in both cases the perils associated with a public good in private hands are palpable. For both short form (Twitter/Mastodon) and longer form (journals) scholarly discourse, decentralized solutions exist, some of which are already enjoying some institutional support. Here we argue that scholarly organizations, in particular learned societies, are now facing a golden opportunity to rethink their hesitations towards such alternatives and support the migration of the scholarly community from Twitter to Mastodon by hosting Mastodon instances. Demonstrating that the scholarly community is capable of creating a truly public square for scholarly discourse, impervious to private takeover, might renew confidence and inspire the community to focus on analogous solutions for the remaining scholarly recordencompassing text, data and code -to safeguard all publicly owned scholarly knowledge.\nA public good in private hands -againWith the turmoil surrounding Elon Musk's handling of his Twitter take-over, the problems associated with a public good in private hands have again become a focus of public attention. For scientists, the situation is not unlike that of 2009, when a social media platform widely used by scholars, FriendFeed, was bought by Facebook and subsequently shut down [1]. This instance was only one of several where the dangers of private, profit-oriented organizations owning platforms used for scholarly discourse became palpable for everyone involved and were widely discussed. One of the outcomes of these discussions over the last 15 years is a set of open standards for social technologies that mimic the open standards underlying the wider internet and web, the World Wide Web Consortium's ActivityPub [2]. In 2009, scholars started to leave FriendFeed and migrate to Twitter, founding what has grown to a community about half a million researchers and is often referred to as #ScienceTwitter [3]. Now, much of #ScienceTwitter is migrating to Mastodon [4], an application based on ActivityPub in what is called the \"Fediverse\" [2]. Analogous to web or email servers, Mastodon runs on so-called instances (servers) and while anybody can implement such instances, nobody can control all of them, just like nobody controls all email or web servers [5]. While corporate capture is a risk even for such decentralized technologies (see, e.g., GMail or Meta's announcement of entering the Fediverse), decentralization provides means for defending against corporate capture. See our companion publication [REF]  for more safeguards against corporate capture. We identify parallels between private ownership of #ScienceTwitter and private ownership of scholarly journals, prompting a proposal to safeguard the entire scholarly record from corporate vagaries.\nA golden opportunityEven before Mr. Musk bought Twitter, especially at-risk scholars of various minority groups were already leaving the increasingly toxic site and founded scholar.social on Mastodon [6]. Now,",
    "acknowledgment": "We are indebted to Hillel Chiel for critical comments at a crucial stage of writing, as well as to Michael Eisen for providing most of the references on scholarly societies attacking modernization. We thank three anonymous reviewers for their helpful suggestions."
  },
  {
    "file": "medical.pdf",
    "title": "An insight of medical students on the use of lectures in the curriculum: A pilot study",
    "abstract": "Many hours of a student's life are spent in a lecture hall, \"the supposed place of education, growth, and understanding.\" Likewise, countless hours are spent by faculty and staff in preparation for these lectures. The objective of the study was to discover the perceptions of medical students on current trends in lecture delivery. The study also arises from the fact that there is a dearth of data in Mauritius.\nMethods:A cross-sectional observational analytic pilot study was conducted via the method of a semi-structured closed-ended questionnaire. A 5-point Likert scale (strongly disagree, disagree, neither agree nor disagree, agree, strongly agree) was used to record the responses. Two male and two female students were chosen via a convenience sampling technique from all the semesters viz. Semester one to semester ten.\nResults:Lectures reduced anxiety with regards to study material was \"strongly agreed\" by 46.4% Indians, whereas 62.5% of South Africans and 50% Mauritians opted the \"agree\" option. As far as faculty as role models were concerned, 15 (37.5%) of students [males 7 (35%); females 8 (40%)] strongly agreed with the option. Preference of structural approach and the correlation with gender was found to be statistically significant P <0.05.\nConclusion:The data both suggest and prove that students are ultimately still in favor of lectures; however, duration, content, and lecture formats were critically appraised. Although the study is a pilot study, which was conducted on a small sample size, the findings can be utilized as baseline data for further investigations.",
    "acknowledgment": "We extend our earnest appreciation to Chairman Mr. RPN Singh and Prof. Namrata Chhabra, Principal In-charge, Prof. Sushil Dawka, Sir Seewoosagur Ramgoolam Medical College, Mauritius, for providing us with immense support and guidance to conduct the research study effectively. We are also thankful to Dr. Brijesh Sathian, Scientist, Department of geriatrics and long-term care, Rumailah Hospital, Doha, Qatar, for generous help. We extend our thanks to those who participated in the research."
  },
  {
    "file": "mixtral.pdf",
    "title": "Mixtral of Experts",
    "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model finetuned to follow instructions, Mixtral 8x7B -Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B -chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",
    "acknowledgment": "We thank the CoreWeave and Scaleway teams for technical support as we trained our models. We are grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working alongside us to make a sparse mixture of experts compatible with TensorRT-LLM."
  },
  {
    "file": "NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "title": "Attention Is All You Need",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. * Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\u2020 Work performed while at Google Brain.\u2021 Work performed while at Google Research.",
    "acknowledgment": "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration."
  },
  {
    "file": "s41746-023-00823-w.pdf",
    "title": "The proposed EU Directives for AI liability leave worrying gaps likely to impact medical AI",
    "abstract": "Two newly proposed Directives impact liability for artificial intelligence in the EU: a Product Liability Directive (PLD) and an AI Liability Directive (AILD). While these proposed Directives provide some uniform liability rules for AI-caused harm, they fail to fully accomplish the EU's goal of providing clarity and uniformity for liability for injuries caused by AI-driven goods and services. Instead, the Directives leave potential liability gaps for injuries caused by some black-box medical AI systems, which use opaque and complex reasoning to provide medical decisions and/or recommendations. Patients may not be able to successfully sue manufacturers or healthcare providers for some injuries caused by these black-box medical AI systems under either EU Member States' strict or fault-based liability laws. Since the proposed Directives fail to address these potential liability gaps, manufacturers and healthcare providers may have difficulty predicting liability risks associated with creating and/or using some potentially beneficial black-box medical AI systems.",
    "acknowledgment": "This work was funded by the European Union (Grant Agreement no. 101057321). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the Health and Digital Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. S.G. also reports grants from the European Union (Grant Agreement no. 101057099), the National Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of Health (Grant Agreement no. 3R01EB027650-03S1), and the Rock Ethics Institute at Penn State University."
  },
  {
    "file": "The_Yan_Report.pdf",
    "title": "Unusual Features of the SARS-CoV-2 Genome Suggesting Sophisticated Laboratory Modification Rather Than Natural Evolution and Delineation of Its Probable Synthetic Route",
    "abstract": "The COVID-19 pandemic caused by the novel coronavirus SARS-CoV-2 has led to over 910,000 deaths worldwide and unprecedented decimation of the global economy. Despite its tremendous impact, the origin of SARS-CoV-2 has remained mysterious and controversial. The natural origin theory, although widely accepted, lacks substantial support. The alternative theory that the virus may have come from a research laboratory is, however, strictly censored on peer-reviewed scientific journals. Nonetheless, SARS-CoV-2 shows biological characteristics that are inconsistent with a naturally occurring, zoonotic virus. In this report, we describe the genomic, structural, medical, and literature evidence, which, when considered together, strongly contradicts the natural origin theory. The evidence shows that SARS-CoV-2 should be a laboratory product created by using bat coronaviruses ZC45 and/or ZXC21 as a template and/or backbone. Building upon the evidence, we further postulate a synthetic route for SARS-CoV-2, demonstrating that the laboratory-creation of this coronavirus is convenient and can be accomplished in approximately six months. Our work emphasizes the need for an independent investigation into the relevant research laboratories. It also argues for a critical look into certain recently published data, which, albeit problematic, was used to support and claim a natural origin of SARS-CoV-2. From a public health perspective, these actions are necessary as knowledge of the origin of SARS-CoV-2 and of how the virus entered the human population are of pivotal importance in the fundamental control of the COVID-19 pandemic as well as in preventing similar, future pandemics.",
    "acknowledgment": "We would like to thank Daoyu Zhang for sharing with us the findings of mutations in the E proteins in different sub-groups of \u03b2 coronaviruses. We also thank all the anonymous scientists and other individuals, who have contributed in uncovering various facts associated with the origin of SARS-CoV-2."
  }
]